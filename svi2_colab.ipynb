{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Wan2.2 SVI Infinity**\n",
        "### **Running a 14B Video Model on a Free Tesla T4**\n",
        "---\n",
        "\n",
        "### **üìù About This Project**\n",
        "This notebook is my attempt to replicate the advanced **Stable Video Infinity (SVI)** workflow on free hardware. Usually, these workflows require massive GPUs (24GB+ VRAM), but I wanted to see if I could engineer it to run on a standard Google Colab T4 (16GB).\n",
        "\n",
        "To achieve this, I implemented several optimizations:\n",
        "* **GGUF Quantization:** Compressing the model to 4-bit to fit in memory.\n",
        "* **Autoregression:** Generating the video in small loops and stitching them together.\n",
        "* **Memory Hacking:** Manually swapping models in and out of VRAM during generation.\n",
        "\n",
        "*This is a learning experiment. It might be slower than professional tools, but it works!* üöÄ\n",
        "\n",
        "### **‚ö†Ô∏è How to Run**\n",
        "1.  **Runtime Check:** Ensure you are connected to a **T4 GPU** (Runtime > Change runtime type).\n",
        "2.  **Step 1:** Select your model quality and run the setup.\n",
        "3.  **Step 2:** Upload a starting image.\n",
        "4.  **Step 3:** Enter your prompts and watch it generate!"
      ],
      "metadata": {
        "id": "header_cell"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_cell",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üõ†Ô∏è Step 1: Initialize Environment & Download Models\n",
        "# @markdown Select your model compression level. **Q4_K_M** is recommended for the T4 GPU.\n",
        "\n",
        "quantization = \"Q4_K_M (Recommended)\" # @param [\"Q4_K_M (Recommended)\", \"Q5_K_M\", \"Q6_K\",\"Q8_0\"]\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# --- 1. System Config ---\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "print(\"‚è≥ Installing dependencies... (This takes about 3 mins)\")\n",
        "!pip install torch==2.6.0 torchvision==0.21.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.29.post2 triton sageattention==1.0.6 imageio av\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "# --- 2. Clone Repositories ---\n",
        "%cd /content\n",
        "if not os.path.exists(\"ComfyUI\"):\n",
        "    !git clone --branch ComfyUI_v0.3.47 https://github.com/Isi-dev/ComfyUI\n",
        "\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "if not os.path.exists(\"ComfyUI_GGUF\"):\n",
        "    !git clone https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "    !pip install -r ComfyUI_GGUF/requirements.txt\n",
        "\n",
        "if not os.path.exists(\"ComfyUI_KJNodes\"):\n",
        "    !git clone --branch kjnv1.1.3 https://github.com/Isi-dev/ComfyUI_KJNodes.git\n",
        "    !pip install -r ComfyUI_KJNodes/requirements.txt\n",
        "\n",
        "# --- 3. Download Models ---\n",
        "models_dir = \"/content/ComfyUI/models\"\n",
        "os.makedirs(f\"{models_dir}/diffusion_models\", exist_ok=True)\n",
        "os.makedirs(f\"{models_dir}/loras\", exist_ok=True)\n",
        "os.makedirs(f\"{models_dir}/vae\", exist_ok=True)\n",
        "os.makedirs(f\"{models_dir}/text_encoders\", exist_ok=True)\n",
        "\n",
        "# Map Selection to Filename\n",
        "quant_map = {\n",
        "    \"Q4_K_M (Recommended)\": \"Q4_K_M\",\n",
        "    \"Q5_K_M\": \"Q5_K_M\",\n",
        "    \"Q6_K\": \"Q6_K\" ,\n",
        "    \"Q8_0\": \"Q8_0\",\n",
        "}\n",
        "selected_quant = quant_map[quantization]\n",
        "\n",
        "print(f\"üì• Downloading Wan2.2 Models ({selected_quant})...\")\n",
        "# We rename them to 'wan_high.gguf' and 'wan_low.gguf' so the script works regardless of selection\n",
        "!aria2c -x 16 -s 16 -k 1M \"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_high_noise_14B_{selected_quant}.gguf\" -d {models_dir}/diffusion_models -o wan_high.gguf\n",
        "!aria2c -x 16 -s 16 -k 1M \"https://huggingface.co/Isi99999/Wan2.2BasedModels/resolve/main/wan2.2_i2v_low_noise_14B_{selected_quant}.gguf\" -d {models_dir}/diffusion_models -o wan_low.gguf\n",
        "\n",
        "print(\"üì• Downloading Helpers (VAE, Encoders)...\")\n",
        "!aria2c -x 16 -s 16 -k 1M \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors\" -d {models_dir}/text_encoders -o umt5_xxl_fp8_e4m3fn_scaled.safetensors\n",
        "!aria2c -x 16 -s 16 -k 1M \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors\" -d {models_dir}/vae -o wan_2.1_vae.safetensors\n",
        "\n",
        "print(\"üì• Downloading LoRAs (Motion & Stability)...\")\n",
        "!aria2c -x 16 -s 16 -k 1M \"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors\" -d {models_dir}/loras -o lightx2v_high.safetensors\n",
        "!aria2c -x 16 -s 16 -k 1M \"https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors\" -d {models_dir}/loras -o lightx2v_low.safetensors\n",
        "\n",
        "try:\n",
        "    !aria2c -x 16 -s 16 -k 1M \"https://huggingface.co/Kijai/WanVideo_SVI_LoRAs/resolve/main/SVI_v2_PRO_Wan2.2-I2V-A14B_HIGH_lora_rank_128_fp16.safetensors\" -d {models_dir}/loras -o svi_pro_high.safetensors\n",
        "    !aria2c -x 16 -s 16 -k 1M \"https://huggingface.co/Kijai/WanVideo_SVI_LoRAs/resolve/main/SVI_v2_PRO_Wan2.2-I2V-A14B_LOW_lora_rank_128_fp16.safetensors\" -d {models_dir}/loras -o svi_pro_low.safetensors\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Note: SVI LoRAs could not be auto-downloaded. The script will run with Lightx2v only.\")\n",
        "\n",
        "# --- 4. Fix Paths ---\n",
        "!touch /content/ComfyUI/custom_nodes/__init__.py\n",
        "!touch /content/ComfyUI/custom_nodes/ComfyUI_GGUF/__init__.py\n",
        "!touch /content/ComfyUI/custom_nodes/ComfyUI_KJNodes/__init__.py\n",
        "\n",
        "clear_output()\n",
        "print(\"‚úÖ Environment Setup Complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_cell",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üñºÔ∏è Step 2: Upload Source Image\n",
        "# @markdown Choose the image you want to animate. The AI will use this as the starting point.\n",
        "\n",
        "display_image = True # @param {type:\"boolean\"}\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "from IPython.display import Image, display\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "os.makedirs(\"/content/ComfyUI/input\", exist_ok=True)\n",
        "image_path = f\"/content/ComfyUI/input/{filename}\"\n",
        "shutil.move(filename, image_path)\n",
        "\n",
        "print(f\"‚úÖ Image Ready: {filename}\")\n",
        "\n",
        "if display_image:\n",
        "    display(Image(filename=image_path, width=300))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gen_cell",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title üé¨ Step 3: Run Generation Loop\n",
        "# @markdown **Instructions:**\n",
        "# @markdown 1. Enter your prompts separated by `|` (Vertical Bar)(each division will determine what each segment will correspond to generate).\n",
        "# @markdown 2. The script will generate one video segment for each prompt.\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import imageio\n",
        "from IPython.display import Video, display\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "# --- Import Logic (Hidden) ---\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "from nodes import *\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
        "try:\n",
        "    from custom_nodes.ComfyUI_KJNodes.nodes.model_optimization_nodes import PathchSageAttentionKJ, WanVideoTeaCacheKJ\n",
        "except ImportError:\n",
        "    sys.path.append('/content/ComfyUI/custom_nodes/ComfyUI_KJNodes')\n",
        "    from nodes.model_optimization_nodes import PathchSageAttentionKJ, WanVideoTeaCacheKJ\n",
        "\n",
        "try:\n",
        "    from comfy_extras.nodes_wan import WanImageToVideo\n",
        "except ImportError:\n",
        "    import importlib\n",
        "    spec = importlib.util.spec_from_file_location(\"nodes_wan\", \"/content/ComfyUI/comfy_extras/nodes_wan.py\")\n",
        "    wan_module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(wan_module)\n",
        "    WanImageToVideo = wan_module.WanImageToVideo\n",
        "\n",
        "# --- Configuration ---\n",
        "prompt_input = \"A cinematic shot of a man walking forward | The man begins to run fast | The man jumps into the air | The man lands safely\" # @param {type:\"string\"}\n",
        "negative_prompt = \"low quality, distortion, morphing, jpeg artifacts, text, watermark\" # @param {type:\"string\"}\n",
        "\n",
        "width = 640 # @param {type:\"integer\"}\n",
        "height = 360 # @param {type:\"integer\"}\n",
        "frames_per_loop = 65 # @param {type:\"integer\"}\n",
        "steps = 10 # @param {type:\"slider\", min:6, max:20}\n",
        "cfg_scale = 1.0 # @param {type:\"number\"}\n",
        "split_ratio = 0.5 # @param {type:\"slider\", min:0.1, max:0.9}\n",
        "seed = 12345 # @param {type:\"integer\"}\n",
        "\n",
        "# --- Helpers ---\n",
        "def cleanup():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def get_last_frame(decoded_tensor):\n",
        "    last_frame_np = (decoded_tensor[-1].cpu().numpy() * 255).astype(np.uint8)\n",
        "    return PILImage.fromarray(last_frame_np)\n",
        "\n",
        "prompt_list = [p.strip() for p in prompt_input.split('|') if p.strip()]\n",
        "loops = len(prompt_list)\n",
        "\n",
        "print(f\"üìù SVI Mode: Running {loops} sequential segments.\")\n",
        "cleanup()\n",
        "\n",
        "current_image_path = image_path\n",
        "all_video_frames = []\n",
        "\n",
        "# --- Execution Loop ---\n",
        "for i in range(loops):\n",
        "    current_prompt = prompt_list[i]\n",
        "    print(f\"\\nüé• SEGMENT {i+1}/{loops}: \\\"{current_prompt}\\\"\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        # 1. Load Support Models\n",
        "        print(\"üîπ Loading Encoders...\")\n",
        "        vae = VAELoader().load_vae(\"wan_2.1_vae.safetensors\")[0]\n",
        "        clip = CLIPLoader().load_clip(\"umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"wan\", \"default\")[0]\n",
        "\n",
        "        # 2. Encode Prompt\n",
        "        clip_enc = CLIPTextEncode()\n",
        "        pos_cond = clip_enc.encode(clip, current_prompt)[0]\n",
        "        neg_cond = clip_enc.encode(clip, negative_prompt)[0]\n",
        "        del clip, clip_enc\n",
        "        cleanup()\n",
        "\n",
        "        # 3. Load Image & Latents\n",
        "        load_img = LoadImage()\n",
        "        src_image = load_img.load_image(os.path.basename(current_image_path))[0]\n",
        "\n",
        "        wan_i2v = WanImageToVideo()\n",
        "        # Forced Keyword Args to prevent TypeErrors\n",
        "        pos_out, neg_out, latent_input = wan_i2v.encode(\n",
        "            positive=pos_cond, negative=neg_cond, vae=vae,\n",
        "            width=int(width), height=int(height), length=int(frames_per_loop),\n",
        "            batch_size=1, start_image=src_image\n",
        "        )\n",
        "\n",
        "        # --- Phase 1: High Noise (Structure) ---\n",
        "        print(f\"üöÄ High Noise Pass (Motion)...\")\n",
        "        # Note: We load 'wan_high.gguf' which was renamed in step 1 based on user selection\n",
        "        model_high = UnetLoaderGGUF().load_unet(\"wan_high.gguf\")[0]\n",
        "        model_high = PathchSageAttentionKJ().patch(model_high, \"auto\")[0]\n",
        "        try:\n",
        "            model_high = WanVideoTeaCacheKJ().patch_teacache(model_high, 0.25, 0.0, 1.0, \"main_device\", \"14B\")[0]\n",
        "        except: pass\n",
        "\n",
        "        lora_loader = LoraLoaderModelOnly()\n",
        "        model_high = lora_loader.load_lora_model_only(model_high, \"lightx2v_high.safetensors\", 1.0)[0]\n",
        "        if os.path.exists(\"/content/ComfyUI/models/loras/svi_pro_high.safetensors\"):\n",
        "             model_high = lora_loader.load_lora_model_only(model_high, \"svi_pro_high.safetensors\", 1.0)[0]\n",
        "\n",
        "        split_step = int(steps * split_ratio)\n",
        "        latent_high = KSamplerAdvanced().sample(\n",
        "            model=model_high, add_noise=\"enable\", noise_seed=seed+i, steps=steps,\n",
        "            cfg=cfg_scale, sampler_name=\"euler\", scheduler=\"simple\",\n",
        "            positive=pos_out, negative=neg_out, latent_image=latent_input,\n",
        "            start_at_step=0, end_at_step=split_step, return_with_leftover_noise=\"enable\"\n",
        "        )[0]\n",
        "        del model_high\n",
        "        cleanup()\n",
        "\n",
        "        # --- Phase 2: Low Noise (Detail) ---\n",
        "        print(f\"üöÄ Low Noise Pass (Texture)...\")\n",
        "        model_low = UnetLoaderGGUF().load_unet(\"wan_low.gguf\")[0]\n",
        "        model_low = PathchSageAttentionKJ().patch(model_low, \"auto\")[0]\n",
        "        try:\n",
        "            model_low = WanVideoTeaCacheKJ().patch_teacache(model_low, 0.25, 0.0, 1.0, \"main_device\", \"14B\")[0]\n",
        "        except: pass\n",
        "\n",
        "        model_low = lora_loader.load_lora_model_only(model_low, \"lightx2v_low.safetensors\", 1.0)[0]\n",
        "        if os.path.exists(\"/content/ComfyUI/models/loras/svi_pro_low.safetensors\"):\n",
        "            model_low = lora_loader.load_lora_model_only(model_low, \"svi_pro_low.safetensors\", 1.0)[0]\n",
        "\n",
        "        final_latent = KSamplerAdvanced().sample(\n",
        "            model=model_low, add_noise=\"disable\", noise_seed=seed+i, steps=steps,\n",
        "            cfg=cfg_scale, sampler_name=\"euler\", scheduler=\"simple\",\n",
        "            positive=pos_out, negative=neg_out, latent_image=latent_high,\n",
        "            start_at_step=split_step, end_at_step=1000, return_with_leftover_noise=\"disable\"\n",
        "        )[0]\n",
        "        del model_low\n",
        "        cleanup()\n",
        "\n",
        "        # --- Decode ---\n",
        "        print(\"üé• Decoding Frames...\")\n",
        "        decoded = VAEDecode().decode(vae, final_latent)[0]\n",
        "        frames_np = (decoded.cpu().numpy() * 255).astype(np.uint8)\n",
        "        all_video_frames.append(frames_np)\n",
        "\n",
        "        if i < loops - 1:\n",
        "            last_img = get_last_frame(decoded)\n",
        "            current_image_path = f\"/content/ComfyUI/input/temp_loop_{i}.png\"\n",
        "            last_img.save(current_image_path)\n",
        "\n",
        "        del vae, decoded, final_latent\n",
        "        cleanup()\n",
        "\n",
        "# --- Final Output ---\n",
        "print(\"\\nüé¨ Stitching final movie...\")\n",
        "full_video = np.concatenate(all_video_frames, axis=0)\n",
        "output_fn = \"/content/output_infinity.mp4\"\n",
        "imageio.mimsave(output_fn, full_video, fps=16)\n",
        "\n",
        "print(f\"‚úÖ Final Video Saved: {output_fn}\")\n",
        "display(Video(output_fn, embed=True))"
      ]
    }
  ]
}